<div align="center">
<h1><b>RAG</b></h1>
<p>This is the source code for a RAG model setting and implementation</p>
</div>

<div align="center">
<img alt="PyPI - Python Version" src="https://img.shields.io/badge/Python-3.9-blue">
<img alt="PyPI - PyToch Version" src="https://img.shields.io/badge/PyTorch-1.11.0-blue">
<img alt="PyPI - PyToch Version" src="https://img.shields.io/badge/Selenium-4.17-blue">
<img alt="PyPI - PyToch Version" src="https://img.shields.io/badge/PostGreSQL-Orange">
<img alt="PyPI - PyToch Version" src="https://img.shields.io/badge/Docker-1C90ED">
<img alt="PyPI - PyToch Version" src="https://img.shields.io/badge/Nginx-Orange">
<img alt="PyPI - PyToch Version" src="https://img.shields.io/badge/Transformers-yellow">
</div>


The term <b>"RAG"</b> in the context you described indeed stands for "Retrieval Augmented Generation."

This concept is a method of enhancing the performance of Large Language Models (LLMs) like GPT-3 or GPT-4 by incorporating a database of document embeddings.

<b>Here's a breakdown of how it works:</b> Database of Document Embeddings: A vector database is created, containing embeddings of various documents. These document embeddings are typically representations of the documents in a high-dimensional vector space, generated using techniques like Word2Vec, Doc2Vec, or more advanced methods like BERT embeddings.

<b>Querying the Database:</b> When a user provides a query or prompt to the LLM, instead of relying solely on the model's pre-trained knowledge, the system also queries this database of document embeddings. The query is used to find semantically similar documents within the database.

<b>Retrieval of Relevant Content:</b> The documents retrieved from the database are considered semantically similar to the user's query. This retrieval process can be based on cosine similarity or other vector-based distance metrics. The retrieved documents are then used as additional context or information for the LLM.

<b>Model Augmentation:</b> The retrieved content from the database is fed to the LLM along with the user's query. This additional information acts as context or input for the model. The LLM can now generate responses that are more informed and contextually relevant based on both its pre-trained knowledge and the retrieved documents.

<b>Enhanced Output:</b> The output generated by the LLM is enhanced with the extra information from the retrieved documents, potentially making the responses more accurate, informative, and contextually appropriate. RAG models are particularly useful in situations where it's important to provide responses that are not solely reliant on the pre-existing knowledge of the LLM but also on specific, up-to-date, or contextually relevant
information present in a document database. This approach can be valuable for tasks like question-answering, content generation, or dialogue systems when a broader context is needed.

<h2>The Idea</h2>
The goal of this project is to offer an AI Assistant that can offer legal information 

<h3> The first steps </h3>
The first milestone is to build a Database from which the model will retrieve the information. For the Database I decided on Neo4j. I think using a knowledge graph will yield a better performance.
The knowledge graph will contain multiple sources for law texts, initially texts from books available on https://www.gesetze-im-internet.de/, with the intent of growing the knowledge base later on. The knowledge graph will also model the relations between the different law texts (i.e. the reference of law texts to others).
To achieve this goal two ML models have been trained on manually annotated datasets to classify and extract reference from the text. The models weights and the training methods will soon be made available as well as the dataset. 
